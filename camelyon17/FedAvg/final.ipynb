{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gris/gris-f/homelv/adatta/miniconda3/envs/pytorchenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "device = torch.device('cuda:2') if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>patient</th>\n",
       "      <th>node</th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "      <th>tumor</th>\n",
       "      <th>slide</th>\n",
       "      <th>center</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3328</td>\n",
       "      <td>21792</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3200</td>\n",
       "      <td>22272</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3168</td>\n",
       "      <td>22272</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3328</td>\n",
       "      <td>21760</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3232</td>\n",
       "      <td>22240</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455949</th>\n",
       "      <td>455949</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>14784</td>\n",
       "      <td>7648</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455950</th>\n",
       "      <td>455950</td>\n",
       "      <td>99</td>\n",
       "      <td>4</td>\n",
       "      <td>3872</td>\n",
       "      <td>11328</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455951</th>\n",
       "      <td>455951</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>31968</td>\n",
       "      <td>9536</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455952</th>\n",
       "      <td>455952</td>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>23712</td>\n",
       "      <td>12192</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455953</th>\n",
       "      <td>455953</td>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>25408</td>\n",
       "      <td>9120</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455954 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  patient  node  x_coord  y_coord  tumor  slide  center  \\\n",
       "0                0        4     4     3328    21792      1      0       0   \n",
       "1                1        4     4     3200    22272      1      0       0   \n",
       "2                2        4     4     3168    22272      1      0       0   \n",
       "3                3        4     4     3328    21760      1      0       0   \n",
       "4                4        4     4     3232    22240      1      0       0   \n",
       "...            ...      ...   ...      ...      ...    ...    ...     ...   \n",
       "455949      455949       88     1    14784     7648      0     45       4   \n",
       "455950      455950       99     4     3872    11328      0     49       4   \n",
       "455951      455951       92     1    31968     9536      0     47       4   \n",
       "455952      455952       81     4    23712    12192      0     41       4   \n",
       "455953      455953       86     4    25408     9120      0     43       4   \n",
       "\n",
       "        split  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "455949      1  \n",
       "455950      0  \n",
       "455951      0  \n",
       "455952      0  \n",
       "455953      0  \n",
       "\n",
       "[455954 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading metadata.csv\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/local/scratch/camelyon17/camelyon17_v1.0/metadata.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "\n",
    "num_workers = 2\n",
    "data_path = Path(\"/local/scratch/camelyon17/camelyon17_v1.0/patches\")\n",
    "batch_size = 32\n",
    "total_samples, num_samples = 0, 0\n",
    "seed = 42\n",
    "num_clients = 5\n",
    "df = pd.read_csv(\"/local/scratch/camelyon17/camelyon17_v1.0/metadata.csv\")\n",
    "num_test_clients = 1\n",
    "\n",
    "\n",
    "def custom_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGBA')\n",
    "\n",
    "\n",
    "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\"Finds the class folder names in a target directory.\n",
    "    \n",
    "    Assumes target directory is in standard image classification format.\n",
    "\n",
    "    Args:\n",
    "        directory (str): target directory to load classnames from.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n",
    "    \n",
    "    Example:\n",
    "        find_classes(\"food_images/train\")\n",
    "        >>> ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n",
    "    \"\"\"\n",
    "    # 1. Get the class names by scanning the target directory\n",
    "    classes = [\"Non-cancerous\", \"Cancerous\"]\n",
    "    \n",
    "    # 2. Raise an error if class names not found\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
    "        \n",
    "    # 3. Create a dictionary of index labels (computers prefer numerical rather than string labels)\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "# Write a custom dataset class (inherits from torch.utils.data.Dataset)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 1. Subclass torch.utils.data.Dataset\n",
    "class ImageFolderCustom(Dataset):\n",
    "    \n",
    "    # 2. Initialize with a targ_dir and transform (optional) parameter\n",
    "    def __init__(self, targ_dir: str, transform=None) -> None:\n",
    "        \n",
    "        # 3. Create class attributes\n",
    "        # Get all image paths\n",
    "        self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.png\")) # note: you'd have to update this if you've got .png's or .jpeg's\n",
    "        self.center = np.empty(shape=(len(self.paths),))\n",
    "        # Setup transforms\n",
    "        df = pd.read_csv(\"/local/scratch/camelyon17/camelyon17_v1.0/metadata.csv\")\n",
    "        for idx in range(len(self.paths)):\n",
    "            image_path = self.paths[idx]\n",
    "            regex = re.compile(r'patch_patient_(\\d+)_node_(\\d+)_x_(\\d+)_y_(\\d+).png')\n",
    "            mo=regex.search(str(image_path)[69:])\n",
    "            patient,node,x,y = mo.groups()\n",
    "            patient,node,x,y = int(patient), int(node), int(x), int(y)\n",
    "            self.center[idx] = int(df[(df[\"patient\"] == patient) & (df[\"node\"] == node) & (df[\"x_coord\"] == x) & (df[\"y_coord\"] == y)][\"center\"].iloc[0])\n",
    "\n",
    "\n",
    "        self.transform = transform\n",
    "        # Create classes and class_to_idx attributes\n",
    "        self.classes, self.class_to_idx = find_classes(targ_dir)\n",
    "\n",
    "    # 4. Make function to load images\n",
    "    def load_image(self, index: int) -> Image.Image:\n",
    "        \"Opens an image via a path and returns it.\"\n",
    "        image_path = self.paths[index]\n",
    "        return Image.open(image_path) \n",
    "    \n",
    "    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.paths)\n",
    "    \n",
    "    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        img = self.load_image(index)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img_arr = np.asarray(img)\n",
    "        img = Image.fromarray(img_arr)\n",
    "        image_path = self.paths[index]\n",
    "        regex = re.compile(r'patch_patient_(\\d+)_node_(\\d+)_x_(\\d+)_y_(\\d+).png')\n",
    "        mo=regex.search(str(image_path)[69:])\n",
    "        # print(mo.groups())\n",
    "        patient,node,x,y = mo.groups()\n",
    "        patient,node,x,y = int(patient), int(node), int(x), int(y)\n",
    "\n",
    "        has_cancer = int(df[(df[\"patient\"] == patient) & (df[\"node\"] == node) & (df[\"x_coord\"] == x) & (df[\"y_coord\"] == y)][\"tumor\"].iloc[0])     \n",
    "        class_name  = \"Cancerous\" if has_cancer else \"Non-cancerous\" # expects path in data_folder/class_name/image.jpeg\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        # Transform if necessary\n",
    "        if self.transform:\n",
    "            return self.transform(img), class_idx # return data, label (X, y)\n",
    "        else:\n",
    "            return img, class_idx # return data, label (X, y)\n",
    "\n",
    "\n",
    "def create_dataloaders(data_transform: transforms.Compose):\n",
    "    random.seed(seed)\n",
    "\n",
    "    data = ImageFolderCustom(\n",
    "        targ_dir = data_path,\n",
    "        transform = data_transform\n",
    "    )\n",
    "\n",
    "    total_samples = len(data)\n",
    "    num_samples = total_samples\n",
    "\n",
    "    indices = torch.tensor(np.random.permutation(np.arange(total_samples)))\n",
    "\n",
    "    train_indices=[]\n",
    "    validate_indices=[]\n",
    "    test_indices=[]\n",
    "\n",
    "    num_samples_per_client = int(num_samples/num_clients)\n",
    "    train_split, val_split = int(0.8*num_samples_per_client), num_samples_per_client - int(0.8*num_samples_per_client)\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        if(i < num_clients - num_test_clients):\n",
    "            train_indices.append(indices[num_samples_per_client*(i): num_samples_per_client*(i) + train_split])\n",
    "            validate_indices.append(indices[num_samples_per_client*(i) + train_split: num_samples_per_client*(i+1)])\n",
    "        else:\n",
    "            test_indices.append(indices[num_samples_per_client*(i): num_samples_per_client*(i + 1)])\n",
    "\n",
    "    train_datasets = []\n",
    "    validate_datasets = []\n",
    "    test_datasets = []\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        if(i < num_clients - num_test_clients):\n",
    "            train_datasets.append(Subset(data, train_indices[i]))\n",
    "            validate_datasets.append(Subset(data, validate_indices[i]))\n",
    "        else:\n",
    "            test_datasets.append(Subset(data, test_indices[i - (num_clients - num_test_clients)]))\n",
    "    \n",
    "    train_dataloaders = []\n",
    "    validate_dataloaders = []\n",
    "    test_dataloaders = []\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        if(i < num_clients - num_test_clients):\n",
    "            train_dataloaders.append(\n",
    "                DataLoader(\n",
    "                    dataset = train_datasets[i],\n",
    "                    batch_size = batch_size,\n",
    "                    shuffle = True,\n",
    "                    num_workers = num_workers,\n",
    "                    pin_memory = True\n",
    "            ))\n",
    "\n",
    "            validate_dataloaders.append(\n",
    "                DataLoader(\n",
    "                    dataset = validate_datasets[i],\n",
    "                    batch_size = batch_size,\n",
    "                    shuffle = True,\n",
    "                    num_workers = num_workers,\n",
    "                    pin_memory = True\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            test_dataloaders.append(\n",
    "                DataLoader(\n",
    "                    dataset = test_datasets[i - (num_clients - num_test_clients)],\n",
    "                    batch_size = batch_size,\n",
    "                    shuffle = True,\n",
    "                    num_workers = num_workers,\n",
    "                    pin_memory = True\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return train_dataloaders, validate_dataloaders, test_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 455954/455954 [10:44<00:00, 707.60it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "paths = list(pathlib.Path(data_path).glob(\"*/*.png\")) # note: you'd have to update this if you've got .png's or .jpeg's\n",
    "# print(len(paths))\n",
    "center = []\n",
    "# # Setup transforms\n",
    "df = pd.read_csv(\"/local/scratch/camelyon17/camelyon17_v1.0/metadata.csv\")\n",
    "regex = re.compile(r'patch_patient_(\\d+)_node_(\\d+)_x_(\\d+)_y_(\\d+).png')\n",
    "for idx in tqdm(range(len(paths))):\n",
    "    image_path = paths[idx]\n",
    "    mo=regex.search(str(image_path)[69:])\n",
    "    patient,node,x,y = mo.groups()\n",
    "    patient,node,x,y = int(patient), int(node), int(x), int(y)\n",
    "    center.append(int(df[(df[\"patient\"] == patient) & (df[\"node\"] == node) & (df[\"x_coord\"] == x) & (df[\"y_coord\"] == y)][\"center\"].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = np.array(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59436,  34904,  85054, 129838, 146722])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91190.8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2*(59436+34904+85054+129838+146722)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mImageFolderCustom\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarg_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 76\u001b[0m, in \u001b[0;36mImageFolderCustom.__init__\u001b[0;34m(self, targ_dir, transform)\u001b[0m\n\u001b[1;32m     74\u001b[0m     patient,node,x,y \u001b[38;5;241m=\u001b[39m mo\u001b[38;5;241m.\u001b[39mgroups()\n\u001b[1;32m     75\u001b[0m     patient,node,x,y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(patient), \u001b[38;5;28mint\u001b[39m(node), \u001b[38;5;28mint\u001b[39m(x), \u001b[38;5;28mint\u001b[39m(y)\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(df[\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatient\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpatient\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_coord\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m x) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_coord\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m y)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Create classes and class_to_idx attributes\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorchenv/lib/python3.8/site-packages/pandas/core/ops/common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     79\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorchenv/lib/python3.8/site-packages/pandas/core/arraylike.py:70\u001b[0m, in \u001b[0;36mOpsMixin.__and__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__and__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__and__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logical_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mand_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorchenv/lib/python3.8/site-packages/pandas/core/series.py:6108\u001b[0m, in \u001b[0;36mSeries._logical_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6105\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   6107\u001b[0m res_values \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mlogical_op(lvalues, rvalues, op)\n\u001b[0;32m-> 6108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorchenv/lib/python3.8/site-packages/pandas/core/series.py:3105\u001b[0m, in \u001b[0;36mSeries._construct_result\u001b[0;34m(self, result, name)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;66;03m# TODO: result should always be ArrayLike, but this fails for some\u001b[39;00m\n\u001b[1;32m   3103\u001b[0m \u001b[38;5;66;03m#  JSONArray tests\u001b[39;00m\n\u001b[1;32m   3104\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 3105\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3106\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   3108\u001b[0m \u001b[38;5;66;03m# Set the result's name after __finalize__ is called because __finalize__\u001b[39;00m\n\u001b[1;32m   3109\u001b[0m \u001b[38;5;66;03m#  would set it back to self.name\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorchenv/lib/python3.8/site-packages/pandas/core/series.py:518\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    515\u001b[0m         data \u001b[38;5;241m=\u001b[39m SingleArrayManager\u001b[38;5;241m.\u001b[39mfrom_array(data, index)\n\u001b[1;32m    517\u001b[0m NDFrame\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data)\n\u001b[0;32m--> 518\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_axis(\u001b[38;5;241m0\u001b[39m, index)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorchenv/lib/python3.8/site-packages/pandas/core/generic.py:6001\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5996\u001b[0m \u001b[38;5;66;03m# first try regular attribute access via __getattribute__, so that\u001b[39;00m\n\u001b[1;32m   5997\u001b[0m \u001b[38;5;66;03m# e.g. ``obj.x`` and ``obj.x = 4`` will always reference/modify\u001b[39;00m\n\u001b[1;32m   5998\u001b[0m \u001b[38;5;66;03m# the same attribute.\u001b[39;00m\n\u001b[1;32m   6000\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 6001\u001b[0m     \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6002\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[1;32m   6003\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorchenv/lib/python3.8/site-packages/pandas/core/series.py:621\u001b[0m, in \u001b[0;36mSeries.name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;66;03m# DataFrame compatibility\u001b[39;00m\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 621\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Hashable:\n\u001b[1;32m    623\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;124;03m    Return the name of the Series.\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;03m    'Even Numbers'\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = ImageFolderCustom(\n",
    "    targ_dir=data_path,\n",
    "    transform=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to find classes in target directory\n",
    "from typing import Dict, List, Tuple\n",
    "import pathlib\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "def custom_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGBA')\n",
    "\n",
    "\n",
    "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\"Finds the class folder names in a target directory.\n",
    "    \n",
    "    Assumes target directory is in standard image classification format.\n",
    "\n",
    "    Args:\n",
    "        directory (str): target directory to load classnames from.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n",
    "    \n",
    "    Example:\n",
    "        find_classes(\"food_images/train\")\n",
    "        >>> ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n",
    "    \"\"\"\n",
    "    # 1. Get the class names by scanning the target directory\n",
    "    classes = [\"Non-cancerous\", \"Cancerous\"]\n",
    "    \n",
    "    # 2. Raise an error if class names not found\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
    "        \n",
    "    # 3. Create a dictionary of index labels (computers prefer numerical rather than string labels)\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "# Write a custom dataset class (inherits from torch.utils.data.Dataset)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 1. Subclass torch.utils.data.Dataset\n",
    "class ImageFolderCustom(Dataset):\n",
    "    \n",
    "    # 2. Initialize with a targ_dir and transform (optional) parameter\n",
    "    def __init__(self, targ_dir: str, transform=None) -> None:\n",
    "        \n",
    "        # 3. Create class attributes\n",
    "        # Get all image paths\n",
    "        self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.png\")) # note: you'd have to update this if you've got .png's or .jpeg's\n",
    "        # Setup transforms\n",
    "        self.transform = transform\n",
    "        # Create classes and class_to_idx attributes\n",
    "        self.classes, self.class_to_idx = find_classes(targ_dir)\n",
    "\n",
    "    # 4. Make function to load images\n",
    "    def load_image(self, index: int) -> Image.Image:\n",
    "        \"Opens an image via a path and returns it.\"\n",
    "        image_path = self.paths[index]\n",
    "        return Image.open(image_path) \n",
    "    \n",
    "    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.paths)\n",
    "    \n",
    "    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        img = self.load_image(index)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img_arr = np.asarray(img)[32:64, 32:64, :]\n",
    "        img = Image.fromarray(img_arr)\n",
    "        image_path = self.paths[index]\n",
    "        regex = re.compile(r'patch_patient_(\\d+)_node_(\\d+)_x_(\\d+)_y_(\\d+).png')\n",
    "        mo=regex.search(str(image_path)[69:])\n",
    "    # print(mo.groups())\n",
    "        patient,node,x,y = mo.groups()\n",
    "        patient,node,x,y = int(patient), int(node), int(x), int(y)\n",
    "\n",
    "        has_cancer = int(df[(df[\"patient\"] == patient) & (df[\"node\"] == node) & (df[\"x_coord\"] == x) & (df[\"y_coord\"] == y)][\"tumor\"].iloc[0])     \n",
    "        class_name  = \"Cancerous\" if has_cancer else \"Non-cancerous\" # expects path in data_folder/class_name/image.jpeg\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        # Transform if necessary\n",
    "        if self.transform:\n",
    "            return self.transform(img), class_idx # return data, label (X, y)\n",
    "        else:\n",
    "            return img, class_idx # return data, label (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchinfo\n",
    "writer = SummaryWriter()\n",
    "\n",
    "def create_writer(experiment_name: str, \n",
    "                  model_name: str, \n",
    "                  extra: str=None) -> torch.utils.tensorboard.writer.SummaryWriter():\n",
    "    \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n",
    "\n",
    "    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n",
    "\n",
    "    Where timestamp is the current date in YYYY-MM-DD format.\n",
    "\n",
    "    Args:\n",
    "        experiment_name (str): Name of experiment.\n",
    "        model_name (str): Name of model.\n",
    "        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n",
    "\n",
    "    Example usage:\n",
    "        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n",
    "        writer = create_writer(experiment_name=\"data_10_percent\",\n",
    "                               model_name=\"effnetb2\",\n",
    "                               extra=\"5_epochs\")\n",
    "        # The above is the same as:\n",
    "        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    # Get timestamp of current date (all experiments on certain day live in same folder)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n",
    "\n",
    "    if extra:\n",
    "        # Create log directory path\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
    "    else:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "        \n",
    "    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices made\n",
      "Datasets made\n",
      "16000 2000 2000\n",
      "DataLoaders made\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"/local/scratch/camelyon17/camelyon17_v1.0/patches\")\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "NUM_SAMPLES=20000\n",
    "total_samples=100000\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "data = ImageFolderCustom(\n",
    "    targ_dir = data_path,\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "indices=torch.tensor(numpy.random.permutation(numpy.arange(total_samples)))\n",
    "\n",
    "train_indices = []\n",
    "validate_indices=[]\n",
    "test_indices = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_indices.append(indices[NUM_SAMPLES*(i):NUM_SAMPLES*(i) + 16000])\n",
    "    validate_indices.append(indices[NUM_SAMPLES*(i) + 16000: NUM_SAMPLES*(i) + 18000])\n",
    "    test_indices.append(indices[NUM_SAMPLES*(i) + 18000: NUM_SAMPLES*(i) + NUM_SAMPLES])\n",
    "\n",
    "print(\"Indices made\")\n",
    "\n",
    "train_datasets = []\n",
    "validate_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_datasets.append(Subset(data, train_indices[i]))\n",
    "    validate_datasets.append(Subset(data, validate_indices[i]))\n",
    "    test_datasets.append(Subset(data, test_indices[i]))\n",
    "\n",
    "print(\"Datasets made\")\n",
    "\n",
    "\n",
    "print(len(train_datasets[0]), len(test_datasets[0]), len(validate_datasets[0]))\n",
    "\n",
    "train_dataloaders = []\n",
    "validate_dataloaders = []\n",
    "test_dataloaders = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_dataloaders.append(DataLoader(\n",
    "        dataset=train_datasets[i],\n",
    "        batch_size = BATCH_SIZE,\n",
    "        shuffle = True,\n",
    "        num_workers= NUM_WORKERS,\n",
    "        pin_memory = True\n",
    "    ))\n",
    "\n",
    "    validate_dataloaders.append(DataLoader(\n",
    "        dataset=validate_datasets[i],\n",
    "        batch_size = BATCH_SIZE,\n",
    "        shuffle = True,\n",
    "        num_workers= NUM_WORKERS,\n",
    "        pin_memory = True\n",
    "    ))\n",
    "\n",
    "    test_dataloaders.append(DataLoader(\n",
    "        dataset=test_datasets[i],\n",
    "        batch_size = BATCH_SIZE,\n",
    "        shuffle = True,\n",
    "        num_workers= NUM_WORKERS,\n",
    "        pin_memory = True\n",
    "    ))\n",
    "\n",
    "print(\"DataLoaders made\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Non-cancerous', 'Cancerous']\n"
     ]
    }
   ],
   "source": [
    "print(data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, \n",
    "                model_type: Type[nn.Module], \n",
    "                loss_fn: Type[nn.Module],\n",
    "                optimizer: Type[torch.optim.Optimizer],\n",
    "                train_dataloader: Type[DataLoader],\n",
    "                validate_dataloader: Type[DataLoader],\n",
    "                test_dataloader: Type[DataLoader],\n",
    "                lr\n",
    "            ):\n",
    "        self.model = model_type().to(device)\n",
    "        self.loss_fn = loss_fn()\n",
    "        self.optimizer = optimizer(self.model.parameters(), lr)\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.validate_dataloader = validate_dataloader\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.lr = lr\n",
    "    \n",
    "    def refresh(self):\n",
    "        self.model = self.model.to(device)\n",
    "        self.loss_fn = self.loss_fn.__class__()\n",
    "        self.optimizer = self.optimizer.__class__(self.model.parameters(), self.lr)\n",
    "    \n",
    "    def train_step(self):\n",
    "        self.model.train()\n",
    "\n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "\n",
    "        for batch, (x,y) in enumerate(self.train_dataloader):\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            y_pred = self.model(x)\n",
    "\n",
    "            loss = self.loss_fn(y_pred, y)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            y_pred_class = torch.argmax(torch.softmax(y_pred, dim = 1), dim = 1)\n",
    "            train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "        \n",
    "        train_loss /= len(self.train_dataloader)\n",
    "        train_acc /= len(self.train_dataloader)\n",
    "        \n",
    "        return train_loss, train_acc\n",
    "\n",
    "    \n",
    "    def validate_step(self):\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        test_loss, test_acc = 0.0,0.0\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "\n",
    "            for batch, (x,y) in enumerate(self.validate_dataloader):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "\n",
    "                y_pred_logits = self.model(x)\n",
    "\n",
    "                loss = self.loss_fn(y_pred_logits, y)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                y_pred_labels = torch.argmax(torch.softmax(y_pred_logits, dim=1), dim=1)\n",
    "                test_acc += (y_pred_labels == y).sum().item()/len(y_pred_labels)\n",
    "\n",
    "        \n",
    "        test_loss /= len(self.validate_dataloader)\n",
    "        test_acc /= len(self.validate_dataloader)\n",
    "\n",
    "    \n",
    "        return test_loss, test_acc\n",
    "\n",
    "\n",
    "    def train(self, epochs, writer: torch.utils.tensorboard.writer.SummaryWriter) -> Dict[str, List]:\n",
    "\n",
    "        results = {\"train_loss\": [],\n",
    "                \"train_acc\": [],\n",
    "                \"test_loss\": [],\n",
    "                \"test_acc\": []\n",
    "        }\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = self.train_step()\n",
    "\n",
    "            test_loss, test_acc = self.validate_step()\n",
    "\n",
    "            # print(\n",
    "            #     f\"Epoch: {epoch+1} | \"\n",
    "            #     f\"train_loss: {train_loss:.4f} | \"\n",
    "            #     f\"train_acc: {train_acc:.4f} | \"\n",
    "            #     f\"test_loss: {test_loss:.4f} | \"\n",
    "            #     f\"test_acc: {test_acc:.4f}\"\n",
    "            # )\n",
    "            results[\"train_loss\"].append(train_loss)\n",
    "            results[\"train_acc\"].append(train_acc)\n",
    "            results[\"test_loss\"].append(test_loss)\n",
    "            results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "            if writer:\n",
    "                # Add results to SummaryWriter\n",
    "                writer.add_scalars(main_tag=\"Loss\", \n",
    "                                tag_scalar_dict={\"train_loss\": train_loss,\n",
    "                                                    \"test_loss\": test_loss},\n",
    "                                global_step=epoch)\n",
    "                writer.add_scalars(main_tag=\"Accuracy\", \n",
    "                                tag_scalar_dict={\"train_acc\": train_acc,\n",
    "                                                    \"test_acc\": test_acc}, \n",
    "                                global_step=epoch)\n",
    "\n",
    "                # Close the writer\n",
    "                writer.close()\n",
    "            else:\n",
    "                pass\n",
    "        ### End new ###\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "class Server:\n",
    "    def __init__(self,\n",
    "        num_clients,\n",
    "        model_type: Type[nn.Module], \n",
    "        loss_fn: Type[nn.Module],\n",
    "        optimizer: Type[torch.optim.Optimizer],\n",
    "        lr\n",
    "        ):\n",
    "\n",
    "        self.NUM_CLIENTS = num_clients\n",
    "        self.model_type = model_type\n",
    "        self.model = model_type().to(device)\n",
    "        self.loss_fn_type = loss_fn\n",
    "        self.optimizer_type = optimizer\n",
    "        train_dataloaders, validate_dataloaders, test_dataloaders = self.make_data(batch_size=32, num_workers=2)\n",
    "        self.clients = []\n",
    "        for i in range(self.NUM_CLIENTS):\n",
    "            new_client = Client(model_type=self.model_type, \n",
    "                                loss_fn = self.loss_fn_type,\n",
    "                                optimizer=self.optimizer_type,\n",
    "                                train_dataloader = train_dataloaders[i],\n",
    "                                validate_dataloader=validate_dataloaders[i],\n",
    "                                test_dataloader=test_dataloaders[i],\n",
    "                                lr=lr)\n",
    "            self.clients.append(new_client)\n",
    "        \n",
    "\n",
    "\n",
    "    def make_data(self,\n",
    "        batch_size,\n",
    "        num_workers\n",
    "        ):\n",
    "\n",
    "        data_path = Path(\"/local/scratch/camelyon17/camelyon17_v1.0/patches\")\n",
    "        BATCH_SIZE = batch_size\n",
    "        NUM_WORKERS = num_workers\n",
    "        NUM_CLIENTS = self.NUM_CLIENTS\n",
    "\n",
    "        data_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        data = ImageFolderCustom(\n",
    "            targ_dir = data_path,\n",
    "            transform=data_transform\n",
    "        )\n",
    "\n",
    "        total_samples=len(data)\n",
    "        NUM_SAMPLES=total_samples/NUM_CLIENTS\n",
    "        \n",
    "        indices=torch.tensor(numpy.random.permutation(numpy.arange(total_samples)))\n",
    "\n",
    "        train_indices = []\n",
    "        validate_indices=[]\n",
    "        test_indices = []\n",
    "\n",
    "        NUM_SAMPLES = int(NUM_SAMPLES)\n",
    "        train_split, val_split, test_split = int(0.8*NUM_SAMPLES), int(0.1*NUM_SAMPLES), int(0.1*NUM_SAMPLES)\n",
    "\n",
    "        for i in range(NUM_CLIENTS):\n",
    "            train_indices.append(indices[NUM_SAMPLES*(i):NUM_SAMPLES*(i) + train_split])\n",
    "            validate_indices.append(indices[NUM_SAMPLES*(i) + train_split: NUM_SAMPLES*(i) + train_split+val_split])\n",
    "            test_indices.append(indices[NUM_SAMPLES*(i) + train_split+val_split: NUM_SAMPLES*(i) + NUM_SAMPLES])\n",
    "\n",
    "        # print(\"Indices made\")\n",
    "\n",
    "        train_datasets = []\n",
    "        validate_datasets = []\n",
    "        test_datasets = []\n",
    "\n",
    "        for i in range(NUM_CLIENTS):\n",
    "            train_datasets.append(Subset(data, train_indices[i]))\n",
    "            validate_datasets.append(Subset(data, validate_indices[i]))\n",
    "            test_datasets.append(Subset(data, test_indices[i]))\n",
    "\n",
    "        # print(\"Datasets made\")\n",
    "\n",
    "\n",
    "        # print(len(train_datasets[0]), len(test_datasets[0]), len(validate_datasets[0]))\n",
    "\n",
    "        train_dataloaders = []\n",
    "        validate_dataloaders = []\n",
    "        test_dataloaders = []\n",
    "\n",
    "        for i in range(NUM_CLIENTS):\n",
    "            train_dataloaders.append(DataLoader(\n",
    "                dataset=train_datasets[i],\n",
    "                batch_size = BATCH_SIZE,\n",
    "                shuffle = True,\n",
    "                num_workers= NUM_WORKERS,\n",
    "                pin_memory = True\n",
    "            ))\n",
    "\n",
    "            validate_dataloaders.append(DataLoader(\n",
    "                dataset=validate_datasets[i],\n",
    "                batch_size = BATCH_SIZE,\n",
    "                shuffle = True,\n",
    "                num_workers= NUM_WORKERS,\n",
    "                pin_memory = True\n",
    "            ))\n",
    "\n",
    "            test_dataloaders.append(DataLoader(\n",
    "                dataset=test_datasets[i],\n",
    "                batch_size = BATCH_SIZE,\n",
    "                shuffle = True,\n",
    "                num_workers= NUM_WORKERS,\n",
    "                pin_memory = True\n",
    "            ))\n",
    "            \n",
    "        return train_dataloaders, validate_dataloaders, test_dataloaders\n",
    "        # print(\"DataLoaders made\")\n",
    "    \n",
    "    def run(self,\n",
    "        server_epochs,\n",
    "        client_epochs,\n",
    "        folder_path):\n",
    "\n",
    "        for epoch in tqdm(range(server_epochs)):\n",
    "            train_loss, validate_loss = 0,0\n",
    "            train_acc, validate_acc = 0,0\n",
    "            for j in range(self.NUM_CLIENTS):\n",
    "                results = self.clients[i].train(epochs = client_epochs, writer = None)\n",
    "                train_loss += results[\"train_loss\"][-1]\n",
    "                validate_loss += results[\"test_loss\"][-1]\n",
    "                train_acc += results[\"train_acc\"][-1]\n",
    "                validate_acc += results[\"test_acc\"][-1]\n",
    "\n",
    "                p = folder_path/f\"models/client-{j}/\"\n",
    "                p.mkdir(parents=True, exist_ok=True)\n",
    "                file_path = p/f\"epoch-{epoch}.pt\"\n",
    "\n",
    "                torch.save(obj=self.clients[j].model.state_dict(), f=file_path)\n",
    "            train_loss /= self.NUM_CLIENTS\n",
    "            validate_loss /= self.NUM_CLIENTS\n",
    "            train_acc /= self.NUM_CLIENTS\n",
    "            validate_acc /= self.NUM_CLIENTS\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1} | \"\n",
    "                f\"train_loss: {train_loss:.4f} | \"\n",
    "                f\"train_acc: {train_acc:.4f} | \"\n",
    "                f\"validate_loss: {validate_loss:.4f} | \"\n",
    "                f\"validate_acc: {validate_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "            self.model.load_state_dict(self.clients[0].model.state_dict())\n",
    "            for j in range(1, self.NUM_CLIENTS):\n",
    "                for(key, value) in self.model.state_dict().items():\n",
    "                    self.model.state_dict()[key].copy_(self.clients[j].model.state_dict()[key] + value)\n",
    "            \n",
    "            for (key, value) in self.model.state_dict().items():\n",
    "                self.model.state_dict()[key].copy_(value/self.NUM_CLIENTS)\n",
    "\n",
    "            for j in range(self.NUM_CLIENTS):\n",
    "                self.clients[j].model.load_state_dict(self.model.state_dict())\n",
    "                self.clients[j].refresh()\n",
    "                \n",
    "            \n",
    "            final_loss, final_acc = 0,0\n",
    "            for client in self.clients:\n",
    "                temp_loss, temp_acc = client.validate_step()\n",
    "                final_loss += temp_loss\n",
    "                final_acc += temp_acc\n",
    "            \n",
    "            final_loss /= self.NUM_CLIENTS\n",
    "            final_acc /= self.NUM_CLIENTS\n",
    "\n",
    "            print(\n",
    "                \"Aggregated Model\"\n",
    "                f\"Epoch: {epoch+1} | \"\n",
    "                f\"validate_loss: {final_loss:.4f} | \"\n",
    "                f\"validate_acc: {final_acc:.4f}\"\n",
    "            )        \n",
    "\n",
    "            p = folder_path/\"models/server/\"\n",
    "            p.mkdir(parents = True, exist_ok = True)\n",
    "            p_file = p/f\"epoch-{epoch}.pt\"\n",
    "            torch.save(obj=self.model.state_dict(), f=p_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random image path: /local/scratch/camelyon17/camelyon17_v1.0/patches/patient_096_node_0/patch_patient_096_node_0_x_36128_y_5664.png\n",
      "Image class: patient_096_node_0\n",
      "Image height: 96\n",
      "Image width: 96\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=96x96>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing random image\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "image_path_list = list(data_path.glob(\"*/*.png\"))\n",
    "random_image_path = random.choice(image_path_list)\n",
    "image_class = random_image_path.parent.stem\n",
    "\n",
    "img = Image.open(random_image_path)\n",
    "\n",
    "print(f\"Random image path: {random_image_path}\")\n",
    "print(f\"Image class: {image_class}\")\n",
    "print(f\"Image height: {img.height}\") \n",
    "print(f\"Image width: {img.width}\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(data.classes)\n",
    "TOTAL_PIXELS = torch.numel(data[0][0])\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.layer1 = nn.Linear(\n",
    "            in_features = TOTAL_PIXELS,\n",
    "            out_features=200    \n",
    "        )\n",
    "        self.layer2 = nn.Linear(\n",
    "            in_features=200,\n",
    "            out_features=200\n",
    "        )\n",
    "        self.classifier = nn.Linear(\n",
    "            in_features = 200,\n",
    "            out_features = NUM_CLASSES\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(torch.relu(self.layer2(torch.relu(self.layer1(self.flatten(x))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0097,  0.0291]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = MultiLayerPerceptron()\n",
    "\n",
    "img = data[0][0]\n",
    "model0(img.unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.6805 | train_acc: 0.6382 | validate_loss: 0.6788 | validate_acc: 0.6264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 1/5 [08:56<35:47, 536.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated ModelEpoch: 1 | validate_loss: 0.6934 | validate_acc: 0.4968\n",
      "Epoch: 2 | train_loss: 0.6913 | train_acc: 0.5956 | validate_loss: 0.6909 | validate_acc: 0.6093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [18:18<27:33, 551.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated ModelEpoch: 2 | validate_loss: 0.6929 | validate_acc: 0.4968\n",
      "Epoch: 3 | train_loss: 0.6906 | train_acc: 0.6049 | validate_loss: 0.6901 | validate_acc: 0.6092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [27:53<18:44, 562.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated ModelEpoch: 3 | validate_loss: 0.6923 | validate_acc: 0.4968\n",
      "Epoch: 4 | train_loss: 0.6897 | train_acc: 0.6119 | validate_loss: 0.6891 | validate_acc: 0.6207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [37:18<09:23, 563.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated ModelEpoch: 4 | validate_loss: 0.6917 | validate_acc: 0.4968\n",
      "Epoch: 5 | train_loss: 0.6887 | train_acc: 0.6190 | validate_loss: 0.6880 | validate_acc: 0.6294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [46:15<00:00, 555.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated ModelEpoch: 5 | validate_loss: 0.6908 | validate_acc: 0.4871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "server = Server(\n",
    "    num_clients=5,\n",
    "    model_type= MultiLayerPerceptron,\n",
    "    loss_fn=nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.SGD,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "server.run(\n",
    "    server_epochs=5,\n",
    "    client_epochs=1,\n",
    "    folder_path = Path(\"MultiLayerPerceptron/client-1-server-5\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(data.classes)\n",
    "class ConvolutionalNN(nn.Module):\n",
    "    def __init__(self, input_channels: int = 3, output_shape: int = NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Conv2d(\n",
    "            in_channels= input_channels,\n",
    "            out_channels= 32,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=2\n",
    "        )\n",
    "\n",
    "        self.conv_block_2 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=64,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=2\n",
    "        )\n",
    "\n",
    "        self.linear_layer = nn.Linear(\n",
    "            in_features= 64*8*8,\n",
    "            out_features = 512,\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(\n",
    "            in_features = 512,\n",
    "            out_features = NUM_CLASSES\n",
    "        )\n",
    "\n",
    "        self.max_pool2d = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.linear_layer(nn.Flatten()(self.max_pool2d(self.conv_block_2(self.max_pool2d(self.conv_block_1(x)))))))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0949, -0.0301]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = ConvolutionalNN(input_channels=3, output_shape = len(data.classes))\n",
    "model1(data[0][0].unsqueeze(dim=0))\n",
    "# len(data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.2602 | train_acc: 0.8948 | validate_loss: 0.4849 | validate_acc: 0.8348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 1/5 [41:09<2:44:36, 2469.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated ModelEpoch: 1 | validate_loss: 0.6921 | validate_acc: 0.5018\n",
      "Epoch: 2 | train_loss: 0.2089 | train_acc: 0.9177 | validate_loss: 0.5489 | validate_acc: 0.8345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [1:20:55<2:01:01, 2420.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated ModelEpoch: 2 | validate_loss: 0.6621 | validate_acc: 0.5447\n",
      "Epoch: 3 | train_loss: 0.1754 | train_acc: 0.9328 | validate_loss: 0.7116 | validate_acc: 0.8248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [2:06:59<1:25:54, 2577.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated ModelEpoch: 3 | validate_loss: 0.6038 | validate_acc: 0.6044\n",
      "Epoch: 4 | train_loss: 0.1585 | train_acc: 0.9408 | validate_loss: 0.7268 | validate_acc: 0.8238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [2:55:04<44:58, 2698.78s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated ModelEpoch: 4 | validate_loss: 0.5687 | validate_acc: 0.6667\n"
     ]
    }
   ],
   "source": [
    "server = Server(\n",
    "    num_clients=5,\n",
    "    model_type= ConvolutionalNN,\n",
    "    loss_fn=nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "server.run(\n",
    "    server_epochs=5,\n",
    "    client_epochs=5,\n",
    "    folder_path = Path(\"ConvoultionalNN/client-5-server-5\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
